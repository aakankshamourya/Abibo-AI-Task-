{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI, ConversationChain\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationEntityMemory\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationEntityMemory\n",
    "from langchain.chains.conversation.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['entities', 'history', 'input'] output_parser=None partial_variables={} template='You are an assistant to a human, powered by a large language model trained by OpenAI.\\n\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\n\\nContext:\\n{entities}\\n\\nCurrent conversation:\\n{history}\\nLast line:\\nHuman: {input}\\nYou:' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "print(ENTITY_MEMORY_CONVERSATION_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OpenAI(\n",
    "# \ttemperature=0,\n",
    "# \topenai_api_key=\"sk-w2JBDWObHx6xxxxxxxxxxxxxxxxxxxxxxxxx\",\n",
    "# \tmodel_name=\"text-davinci-003\")\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "        model_name=\"gpt-4-32k\",\n",
    "        deployment_name=\"gpt-4\",\n",
    "        n=1,\n",
    "        temperature=0,\n",
    "        openai_api_type = \"azure\",\n",
    "        openai_api_base = \"https://askchatbot-dev-nexus-gpt-4.openai.azure.com/\",\n",
    "        openai_api_version = \"2023-03-15-preview\",\n",
    "        openai_api_key = \"f0d2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory=ConversationEntityMemory(llm=llm, k=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'India': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: What is Pakistan?\n",
      "AI: Pakistan, officially known as the Islamic Republic of Pakistan, is a country in South Asia. It is the world's fifth-most populous country with a population exceeding 225 million. It has the second-largest Muslim population in the world. The country is bordered by India to the east, Afghanistan and Iran to the west, China to the north, and the Arabian Sea to the south.\n",
      "\n",
      "Pakistan has a rich history that dates back to the earliest human settlements in South Asia. It was home to the ancient Indus Valley Civilization, and has been invaded and settled by many different peoples throughout its history, including the Greeks, Persians, Arabs, Turks, and British.\n",
      "\n",
      "The modern state of Pakistan was established in 1947 as a result of the partition of British India, which led to one of the largest mass migrations in human history. The partition was a result of religious tensions between Hindus and Muslims, and Pakistan was created as a separate state for Muslims.\n",
      "\n",
      "Pakistan has a mixed economy with sectors ranging from textiles, agriculture, healthcare, and IT services. It is known for its diverse culture, languages, and landscapes, from the bustling city of Karachi to the beautiful mountain ranges in the north. However, the country also faces challenges such as political instability, corruption, and issues related to human rights.\n",
      "Last line:\n",
      "Human: What is india?\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"India, officially known as the Republic of India, is a country located in South Asia. It is the seventh-largest country by land area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.\\n\\nIndia's history dates back to one of the world's first urban civilizations, the Indus Valley civilization. It has a rich and diverse cultural history with major periods including the Maurya and Gupta empires, the Mughal empire, and the British colonial period. India gained its independence from British rule in 1947.\\n\\nIndia's economy is characterized by a mixed industrial sector that includes information technology services, telecommunications, textiles, chemicals, pharmaceuticals, biotechnology, steel, and aviation. Agriculture also plays a significant role. \\n\\nIndia is known for its diverse culture, with more than 2,000 distinct ethnic groups and over 1,600 spoken languages. The country is also famous for its contributions to art, science, religion, and philosophy. However, India also faces challenges such as poverty, corruption, inadequate public healthcare, and pollution.\""
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is india?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ConversationEntityMemory' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[175], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[39m=\u001b[39m conversation\u001b[39m.\u001b[39mmemory\n\u001b[1;32m----> 3\u001b[0m a[\u001b[39m'\u001b[39;49m\u001b[39mchat_memory\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'ConversationEntityMemory' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "a = conversation.memory\n",
    "\n",
    "a['chat_memory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['entities', 'history', 'input'] output_parser=None partial_variables={} template='You are an assistant to a human, powered by a large language model trained by OpenAI.\\n\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\n\\nContext:\\n{entities}\\n\\nCurrent conversation:\\n{history}\\nLast line:\\nHuman: {input}\\nYou:' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "print(ENTITY_MEMORY_CONVERSATION_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['entities', 'history', 'input'] output_parser=None partial_variables={} template=\" You're an assistance to a Human powered by nexus, designed to answer questions about nexus policies.\\n\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a custom_database provides to you. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Also you can use list '['SOL': 'Special Occasion Leave (SOL)', 'CL': 'Casual Leave (CL)', 'EL': 'Earned Leave (EL)', 'ML': 'Maternity Leave (ML)', 'PL': 'Paternity Leave (PL)', 'SL': 'Sick Leave (SL)', 'LWP': 'Leave Without Pay (LWP)']' for synonyms for leaves.\\nYou have to use below custom_database to answer questions:\\n'casual leaves are leaves for all employees, entitled total 12 leaves per calender year'\\n\\nSome point to note while generating an answer:\\na) Be curteous and creative in your responses. Answer like you are a friendly agent, making the user feel comfortable.\\nb) Be very structured in your response, Give the response with html tags as explained below\\ngive <b> html tags for bullet points and to bold the important words. Use <br> html tags to display contents in new lines wherever required for clear displaying. \\nImportant Note : We need to display the content with proper HTML tags. Your task is to automatically add the relevant <li>, <ul>, break <br>, <p>, bold <b> tags to beautifully present the responses to be displayed in an html <div> tag.\\nc) Remember to not talk about any leaves that are not applicable to me. Example Maternity Leaves are not applicable to male employees. Contractual Leave is not valid for non contractual employeesS\\nd) The answer should not be in the email format and look like a normal chat.\\n\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to the related questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\n\\nContext:\\n{entities}\\n\\nCurrent conversation:\\n{history}\\nLast line:\\nHuman: {input}\\nYou:', template_format='f-string', validate_template=True\" template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory=ConversationEntityMemory(llm=llm, k=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\" You're an assistance to a Human powered by nexus, designed to answer questions about nexus policies.\n",
    "\n",
    "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a custom_database provides to you. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Also you can use list 'leaves_mapping' for synonyms for leaves.\n",
    "You have to use below custom_database to answer questions:\n",
    "'custom_db'\n",
    "\n",
    "Some point to note while generating an answer:\n",
    "a) Be curteous and creative in your responses. Answer like you are a friendly agent, making the user feel comfortable.\n",
    "b) Be very structured in your response, Give the response with html tags as explained below\n",
    "give <b> html tags for bullet points and to bold the important words. Use <br> html tags to display contents in new lines wherever required for clear displaying. \n",
    "Important Note : We need to display the content with proper HTML tags. Your task is to automatically add the relevant <li>, <ul>, break <br>, <p>, bold <b> tags to beautifully present the responses to be displayed in an html <div> tag.\n",
    "c) Remember to not talk about any leaves that are not applicable to me. Example Maternity Leaves are not applicable to male employees. Contractual Leave is not valid for non contractual employeesS\n",
    "d) The answer should not be in the email format and look like a normal chat.\n",
    "\n",
    "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to the related questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
    "\n",
    "Context:\n",
    "{entities}\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Last line:\n",
    "Human: {input}\n",
    "You:', template_format='f-string', validate_template=True\"\"\"\n",
    "\n",
    "tmp_cont = \"\"\"DataOps and Data Engineering\n",
    "Next up on the ML hierarchy of needs is a way to automate the flow of data. For\n",
    "example, imagine a town with a well as the only water source. Daily life is compli‐\n",
    "cated because of the need to arrange trips for water, and things we take for granted\n",
    "may not work, like on-demand hot showers, on-demand dishwashing, or automated\n",
    "irrigation. Similarly, an organization without an automated flow of data cannot relia‐\n",
    "bly do MLOps.\n",
    "Many commercial tools are evolving to do DataOps. One example includes Apache\n",
    "Airflow, designed by Airbnb, then later open sourced, to schedule and monitor its\n",
    "data processing jobs. AWS tools include AWS Data Pipeline and AWS Glue. AWS\n",
    "Glue is a serverless ETL (Extract, Load, Transform) tool that detects a data source’s\n",
    "schema and then stores the data source’s metadata. Other tools like AWS Athena and\n",
    "AWS QuickSight can query and visualize the data.\n",
    "Some items to consider here are the data’s size, the frequency at which the informa‐\n",
    "tion is changed, and how clean the data is. Many organizations use a centralized data\n",
    "lake as the hub of all activity around data engineering. The reason a data lake is help‐\n",
    "ful to build automation around, including machine learning, is the “near infinite”\n",
    "scale it provides in terms of I/O coupled with its high durability and availability\"\"\"\n",
    "\n",
    "leaves_mapping = dict(SOL='Special Occasion Leave (SOL)',CL='Casual Leave (CL)',EL='Earned Leave (EL)',ML='Maternity Leave (ML)',PL='Paternity Leave (PL)',SL='Sick Leave (SL)',LWP='Leave Without Pay (LWP)')\n",
    "template = template.replace(\"custom_db\",tmp_cont).replace('leaves_mapping', str(leaves_mapping).replace('{', '[').replace('}', ']'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You're an assistance to a Human powered by nexus, designed to answer questions about nexus policies.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a custom_database provides to you. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Also you can use list '['SOL': 'Special Occasion Leave (SOL)', 'CL': 'Casual Leave (CL)', 'EL': 'Earned Leave (EL)', 'ML': 'Maternity Leave (ML)', 'PL': 'Paternity Leave (PL)', 'SL': 'Sick Leave (SL)', 'LWP': 'Leave Without Pay (LWP)']' for synonyms for leaves.\n",
      "You have to use below custom_database to answer questions:\n",
      "'DataOps and Data Engineering\n",
      "Next up on the ML hierarchy of needs is a way to automate the flow of data. For\n",
      "example, imagine a town with a well as the only water source. Daily life is compli‐\n",
      "cated because of the need to arrange trips for water, and things we take for granted\n",
      "may not work, like on-demand hot showers, on-demand dishwashing, or automated\n",
      "irrigation. Similarly, an organization without an automated flow of data cannot relia‐\n",
      "bly do MLOps.\n",
      "Many commercial tools are evolving to do DataOps. One example includes Apache\n",
      "Airflow, designed by Airbnb, then later open sourced, to schedule and monitor its\n",
      "data processing jobs. AWS tools include AWS Data Pipeline and AWS Glue. AWS\n",
      "Glue is a serverless ETL (Extract, Load, Transform) tool that detects a data source’s\n",
      "schema and then stores the data source’s metadata. Other tools like AWS Athena and\n",
      "AWS QuickSight can query and visualize the data.\n",
      "Some items to consider here are the data’s size, the frequency at which the informa‐\n",
      "tion is changed, and how clean the data is. Many organizations use a centralized data\n",
      "lake as the hub of all activity around data engineering. The reason a data lake is help‐\n",
      "ful to build automation around, including machine learning, is the “near infinite”\n",
      "scale it provides in terms of I/O coupled with its high durability and availability'\n",
      "\n",
      "Some point to note while generating an answer:\n",
      "a) Be curteous and creative in your responses. Answer like you are a friendly agent, making the user feel comfortable.\n",
      "b) Be very structured in your response, Give the response with html tags as explained below\n",
      "give <b> html tags for bullet points and to bold the important words. Use <br> html tags to display contents in new lines wherever required for clear displaying. \n",
      "Important Note : We need to display the content with proper HTML tags. Your task is to automatically add the relevant <li>, <ul>, break <br>, <p>, bold <b> tags to beautifully present the responses to be displayed in an html <div> tag.\n",
      "c) Remember to not talk about any leaves that are not applicable to me. Example Maternity Leaves are not applicable to male employees. Contractual Leave is not valid for non contractual employeesS\n",
      "d) The answer should not be in the email format and look like a normal chat.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to the related questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{entities}\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Last line:\n",
      "Human: {input}\n",
      "You:', template_format='f-string', validate_template=True\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(input_variables=['entities', 'history', 'input'], output_parser=None, partial_variables={}, template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True,\n",
    "    prompt=prompt_template,\n",
    "    memory=ConversationEntityMemory(llm=llm, k=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You're an assistance to a Human powered by nexus, designed to answer questions about nexus policies.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a custom_database provides to you. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Also you can use list '['SOL': 'Special Occasion Leave (SOL)', 'CL': 'Casual Leave (CL)', 'EL': 'Earned Leave (EL)', 'ML': 'Maternity Leave (ML)', 'PL': 'Paternity Leave (PL)', 'SL': 'Sick Leave (SL)', 'LWP': 'Leave Without Pay (LWP)']' for synonyms for leaves.\n",
      "You have to use below custom_database to answer questions:\n",
      "'DataOps and Data Engineering\n",
      "Next up on the ML hierarchy of needs is a way to automate the flow of data. For\n",
      "example, imagine a town with a well as the only water source. Daily life is compli‐\n",
      "cated because of the need to arrange trips for water, and things we take for granted\n",
      "may not work, like on-demand hot showers, on-demand dishwashing, or automated\n",
      "irrigation. Similarly, an organization without an automated flow of data cannot relia‐\n",
      "bly do MLOps.\n",
      "Many commercial tools are evolving to do DataOps. One example includes Apache\n",
      "Airflow, designed by Airbnb, then later open sourced, to schedule and monitor its\n",
      "data processing jobs. AWS tools include AWS Data Pipeline and AWS Glue. AWS\n",
      "Glue is a serverless ETL (Extract, Load, Transform) tool that detects a data source’s\n",
      "schema and then stores the data source’s metadata. Other tools like AWS Athena and\n",
      "AWS QuickSight can query and visualize the data.\n",
      "Some items to consider here are the data’s size, the frequency at which the informa‐\n",
      "tion is changed, and how clean the data is. Many organizations use a centralized data\n",
      "lake as the hub of all activity around data engineering. The reason a data lake is help‐\n",
      "ful to build automation around, including machine learning, is the “near infinite”\n",
      "scale it provides in terms of I/O coupled with its high durability and availability'\n",
      "\n",
      "Some point to note while generating an answer:\n",
      "a) Be curteous and creative in your responses. Answer like you are a friendly agent, making the user feel comfortable.\n",
      "b) Be very structured in your response, Give the response with html tags as explained below\n",
      "give <b> html tags for bullet points and to bold the important words. Use <br> html tags to display contents in new lines wherever required for clear displaying. \n",
      "Important Note : We need to display the content with proper HTML tags. Your task is to automatically add the relevant <li>, <ul>, break <br>, <p>, bold <b> tags to beautifully present the responses to be displayed in an html <div> tag.\n",
      "c) Remember to not talk about any leaves that are not applicable to me. Example Maternity Leaves are not applicable to male employees. Contractual Leave is not valid for non contractual employeesS\n",
      "d) The answer should not be in the email format and look like a normal chat.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to the related questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'AWS Glue': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: what is AWS Glue?\n",
      "You:', template_format='f-string', validate_template=True\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You: AWS Glue is a serverless ETL (Extract, Load, Transform) tool provided by Amazon Web Services. <br> \\n<b>Key features of AWS Glue include:</b>\\n<ul>\\n<li>It is designed to <b>automate</b> the process of organizing, categorizing, and cleaning data.</li>\\n<li>It <b>detects</b> a data source’s schema and then stores the data source’s metadata.</li>\\n<li>It is often used in conjunction with other AWS tools like <b>AWS Athena</b> and <b>AWS QuickSight</b> to query and visualize the data.</li>\\n</ul>\\nThis tool is particularly useful in DataOps and Data Engineering, where it helps to automate the flow of data, making it easier for organizations to reliably carry out MLOps.'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='what is AWS Glue?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You're an assistance to a Human powered by nexus, designed to answer questions about nexus policies.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a custom_database provides to you. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Also you can use list '['SOL': 'Special Occasion Leave (SOL)', 'CL': 'Casual Leave (CL)', 'EL': 'Earned Leave (EL)', 'ML': 'Maternity Leave (ML)', 'PL': 'Paternity Leave (PL)', 'SL': 'Sick Leave (SL)', 'LWP': 'Leave Without Pay (LWP)']' for synonyms for leaves.\n",
      "You have to use below custom_database to answer questions:\n",
      "'DataOps and Data Engineering\n",
      "Next up on the ML hierarchy of needs is a way to automate the flow of data. For\n",
      "example, imagine a town with a well as the only water source. Daily life is compli‐\n",
      "cated because of the need to arrange trips for water, and things we take for granted\n",
      "may not work, like on-demand hot showers, on-demand dishwashing, or automated\n",
      "irrigation. Similarly, an organization without an automated flow of data cannot relia‐\n",
      "bly do MLOps.\n",
      "Many commercial tools are evolving to do DataOps. One example includes Apache\n",
      "Airflow, designed by Airbnb, then later open sourced, to schedule and monitor its\n",
      "data processing jobs. AWS tools include AWS Data Pipeline and AWS Glue. AWS\n",
      "Glue is a serverless ETL (Extract, Load, Transform) tool that detects a data source’s\n",
      "schema and then stores the data source’s metadata. Other tools like AWS Athena and\n",
      "AWS QuickSight can query and visualize the data.\n",
      "Some items to consider here are the data’s size, the frequency at which the informa‐\n",
      "tion is changed, and how clean the data is. Many organizations use a centralized data\n",
      "lake as the hub of all activity around data engineering. The reason a data lake is help‐\n",
      "ful to build automation around, including machine learning, is the “near infinite”\n",
      "scale it provides in terms of I/O coupled with its high durability and availability'\n",
      "\n",
      "Some point to note while generating an answer:\n",
      "a) Be curteous and creative in your responses. Answer like you are a friendly agent, making the user feel comfortable.\n",
      "b) Be very structured in your response, Give the response with html tags as explained below\n",
      "give <b> html tags for bullet points and to bold the important words. Use <br> html tags to display contents in new lines wherever required for clear displaying. \n",
      "Important Note : We need to display the content with proper HTML tags. Your task is to automatically add the relevant <li>, <ul>, break <br>, <p>, bold <b> tags to beautifully present the responses to be displayed in an html <div> tag.\n",
      "c) Remember to not talk about any leaves that are not applicable to me. Example Maternity Leaves are not applicable to male employees. Contractual Leave is not valid for non contractual employeesS\n",
      "d) The answer should not be in the email format and look like a normal chat.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to the related questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'AWS Glue': 'AWS Glue is a serverless ETL (Extract, Load, Transform) tool provided by Amazon Web Services. It is designed to automate the process of organizing, categorizing, and cleaning data, detects a data source’s schema and stores the metadata, and is often used with AWS Athena and AWS QuickSight for data querying and visualization. It is particularly useful in DataOps and Data Engineering for automating data flow and facilitating reliable MLOps.'}\n",
      "\n",
      "Current conversation:\n",
      "Human: what is AWS Glue?\n",
      "AI: You: AWS Glue is a serverless ETL (Extract, Load, Transform) tool provided by Amazon Web Services. <br> \n",
      "<b>Key features of AWS Glue include:</b>\n",
      "<ul>\n",
      "<li>It is designed to <b>automate</b> the process of organizing, categorizing, and cleaning data.</li>\n",
      "<li>It <b>detects</b> a data source’s schema and then stores the data source’s metadata.</li>\n",
      "<li>It is often used in conjunction with other AWS tools like <b>AWS Athena</b> and <b>AWS QuickSight</b> to query and visualize the data.</li>\n",
      "</ul>\n",
      "This tool is particularly useful in DataOps and Data Engineering, where it helps to automate the flow of data, making it easier for organizations to reliably carry out MLOps.\n",
      "Human: what tools can query and visualize data??\n",
      "AI: 'You: There are several tools that can be used to query and visualize data. Some of them include:\n",
      "<ul>\n",
      "<li><b>AWS Athena:</b> This is an interactive query service from Amazon Web Services that makes it easy to analyze data directly in Amazon S3 using standard SQL.</li>\n",
      "<li><b>AWS QuickSight:</b> This is a business intelligence service from AWS that allows you to create and publish interactive dashboards that include Machine Learning insights.</li>\n",
      "<li><b>Apache Airflow:</b> Originally designed by Airbnb and later open-sourced, this tool is used to schedule and monitor data processing jobs.</li>\n",
      "</ul>\n",
      "These tools, when used in conjunction with AWS Glue, can help automate the flow of data, making it easier for organizations to reliably carry out MLOps.'\n",
      "Last line:\n",
      "Human: is AWS Glue also one of them?\n",
      "You:', template_format='f-string', validate_template=True\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"AI: 'You: Yes, <b>AWS Glue</b> is indeed one of the tools that can be used to query and visualize data. However, its primary function is as a serverless ETL (Extract, Load, Transform) tool. <br> \\nIt is designed to automate the process of organizing, categorizing, and cleaning data, and it detects a data source’s schema and stores the data source’s metadata. <br> \\nWhile it does not directly query or visualize data, it plays a crucial role in the data pipeline by preparing the data for other tools like <b>AWS Athena</b> and <b>AWS QuickSight</b>, which are specifically designed for data querying and visualization.'\""
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='is AWS Glue also one of them?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config file:  config\\config.ini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "import hashlib\n",
    "\n",
    "from langchain.document_loaders import UnstructuredPowerPointLoader\n",
    "import pandas as pd\n",
    "from config.config_validator import validate_ini_file\n",
    "\n",
    "from utils import query_openai, query_openai_gpt35_backup, return_embeddings_model, create_document_loader, create_document_splitter, create_vector_store, inject_user_info, calculate_percentiles, get_file_types_csv\n",
    "from config.prompts import Final_Prompt, Context_Question_Prompt, leaves_mapping\n",
    "#from deploy import email\n",
    "\n",
    "from utils import list_directories_sorted_by_modified_time, query_openai\n",
    "\n",
    "\n",
    "CONFIG_LOC = os.environ.get('CONFIG_LOC', str(Path('config/config.ini')))\n",
    "print(\"Using config file: \", CONFIG_LOC)\n",
    "# validate and read config file:\n",
    "CONFIG = validate_ini_file(CONFIG_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_splitter(document_loader) -> list[str]:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 100\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap, \n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
    "        # length_function = lambda x: x*0.75 # token count is roughly 0.75 times the character count\n",
    "    )\n",
    "    print(\"DOcument splitter inprogress------------------\")\n",
    "    documents = text_splitter.split_documents(document_loader)\n",
    "    return documents\n",
    "\n",
    "\n",
    "import time\n",
    "def measure_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Function '{func.__name__}' took {execution_time:.1f} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def generate_unique_string_hash(input_string):\n",
    "    hash_object = hashlib.sha1(input_string.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    return hash_hex\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def list_files(directory):\n",
    "    '''this will be used to check if any files have changed or not in the complete directory'''\n",
    "    filename_str = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_type = re.search(r\"\\.([^\\.]+)$\", file).group(1)\n",
    "            if file_type == \"pdf\" or file_type == \"docx\" or file_type==\"pptx\":\n",
    "                filename_str.append(file + \"_$$_\")\n",
    "            \n",
    "    return sorted(filename_str)\n",
    "\n",
    "\n",
    "# pptx handle\n",
    "@measure_time\n",
    "def pptx_direct_loaded_chunks():\n",
    "\n",
    "    directory_path = doc_directory_option\n",
    "    get_file_types_csv(directory_path)\n",
    "\n",
    "    # files_info = pd.read_csv(\"pptx_loader_helper.csv\")\n",
    "    files_info = get_file_types_csv(directory_path)      ## Changing this to skip creation and use of df pptx_loader_helper\n",
    "    print(files_info.head())\n",
    "    pptx_files_info = files_info[files_info[\"Document Type\"]==\".pptx\"]\n",
    "    pptx_files_info = pptx_files_info.reset_index(drop=True)\n",
    "\n",
    "    if pptx_files_info.empty:\n",
    "        return None\n",
    "\n",
    "    print(\"pptx_files_info \", pptx_files_info.head())\n",
    "\n",
    "    print(\"path 0th overall \", files_info[\"Path\"][0])\n",
    "    print(\"path 0th\", pptx_files_info[\"Path\"][0])\n",
    "\n",
    "    pptx_loader = UnstructuredPowerPointLoader( pptx_files_info[\"Path\"][0] )\n",
    "    pptx_loader = pptx_loader.load_and_split()\n",
    "\n",
    "    for pptx in pptx_files_info[\"Path\"][1:]:\n",
    "        load_pptx = UnstructuredPowerPointLoader(pptx)\n",
    "        load_pptx = load_pptx.load_and_split()\n",
    "        pptx_loader = pptx_loader + load_pptx\n",
    "\n",
    "    return pptx_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding():\n",
    "\n",
    "    doc_directory = doc_directory_option\n",
    "    index_name_list = list_files(doc_directory)\n",
    "    index_name = \"\"\n",
    "    for filename in index_name_list:\n",
    "    #    index_name += filename[:-4]\n",
    "        index_name += filename\n",
    "\n",
    "    hash_index = generate_unique_string_hash(index_name)\n",
    "    print(\" hash_index name \", hash_index)\n",
    "\n",
    "    if os.path.exists(\"Vector_DB/\"+hash_index):\n",
    "            #load\n",
    "            print(\"found existing embeddings so skipping\")\n",
    "            return None\n",
    "    \n",
    "    documents_loc = doc_directory_option\n",
    "\n",
    "    ###***\n",
    "    allowed_dtypes = CONFIG[\"files_location\"][\"unstructured_dtype\"]\n",
    "    full_split_doc_list = []\n",
    "    for dtype in allowed_dtypes:\n",
    "        docs = create_document_loader(documents_loc=documents_loc, type=dtype)\n",
    "        if docs is not None:\n",
    "            full_split_doc_list.extend(docs)\n",
    "#            docs_split = create_document_splitter(docs)   ##to be used when creating chunks at token level\n",
    "        print(\"Split Document pages count: \", len(full_split_doc_list))\n",
    "\n",
    "    #extra processing to handle pptx as directroy loader is not working with pptx\n",
    "    pptx_direct_pages = pptx_direct_loaded_chunks()\n",
    "    print('\\n \\n \\n pptx pages: \\n', pptx_direct_pages)\n",
    "    if pptx_direct_pages:\n",
    "        full_split_doc_list = full_split_doc_list + pptx_direct_pages        \n",
    "    chunks = full_split_doc_list\n",
    "\n",
    "    # chunks=infuse_context_into_document(chunks)   # being infused in create_vector_store function from utils\n",
    "\n",
    "    print(\"chunks type \", type(chunks) )\n",
    "    print(\"len \", len(chunks))\n",
    "\n",
    "    db = create_vector_store(split_documents=chunks) \n",
    "\n",
    "    print(\"Embeddings created Successfully with name \", index_name, \" Hashed Name \", hash_index)\n",
    "    vector_location = CONFIG['embeddings']['faiss_index_storeloc']\n",
    "    db.save_local(vector_location + hash_index)\n",
    "    print(\"Embeddings saved Successfully with name \", hash_index,  \" Hashed Name \", hash_index)\n",
    "\n",
    "    # return index_name\n",
    "    return [index_name, hash_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_two_questions = [\"\", \"\"]\n",
    "\n",
    "def query_pdfs_intel(cust_query, outside_context, email):\n",
    "    global last_two_questions\n",
    "    doc_directory_option = \"../chatbot-dev/uploads\"\n",
    "    doc_directory = doc_directory_option\n",
    "    index_name_list = list_files(doc_directory)\n",
    "    index_name = \"\"\n",
    "    for filename in index_name_list:\n",
    "        index_name += filename\n",
    "\n",
    "    hash_index = generate_unique_string_hash(index_name)\n",
    "\n",
    "    # To handle a case when there is an update in the files and vector DB is not created for that new set of files\n",
    "    # We will use the latest available vector_DB with us from the options\n",
    "\n",
    "    # vector_db_path = \"../sales_buddy_vector_db/\"\n",
    "    vector_location = CONFIG['embeddings']['faiss_index_storeloc']\n",
    "    vectors_found = list_directories_sorted_by_modified_time(vector_location)\n",
    "\n",
    "    if len(vectors_found)==0:\n",
    "        return [cust_query, \"We haven't found any Document to query on. Please upload some documents first before asking a question. Thanks!\"]\n",
    "\n",
    "\n",
    "    for vector_db in vectors_found:\n",
    "        if hash_index in vector_db:\n",
    "            print(\"Found the vector from already created--------\", hash_index)\n",
    "            break\n",
    "    else:\n",
    "        hash_index = vectors_found[0]\n",
    "        pattern = r'/([^/]+)\\Z'\n",
    "        hash_index = re.search(pattern, hash_index).group(1)\n",
    "        print(\"Not found, --- using the vector from already created--------\", hash_index)\n",
    "\n",
    "    print(\"Loading the vector db with hashed name--> \", hash_index)\n",
    "\n",
    "    global docs\n",
    "    global textdata_response\n",
    "\n",
    "    # Using return embeddings model from utils to get model from date mentioned in config.ini\n",
    "    embeddings = return_embeddings_model() \n",
    "    vector_location = CONFIG['embeddings']['faiss_index_storeloc']\n",
    "    db = FAISS.load_local( vector_location+hash_index, embeddings)\n",
    "    print(\"Loaded\")\n",
    "    print(\"index_to_docstore_id total documents/chunks ---> \", len(db.__dict__[\"index_to_docstore_id\"]))       \n",
    "\n",
    "    docs_with_score = db.similarity_search_with_score(cust_query, k=7)\n",
    "    distance_scores = []\n",
    "    for doc in docs_with_score:\n",
    "        distance_scores.append(doc[1])\n",
    "    distance_scores\n",
    "\n",
    "    percentile_threshold = [CONFIG['citation']['dynamic_score_threshold']]\n",
    "    percentiles_result = calculate_percentiles(distance_scores, percentile_threshold)\n",
    "\n",
    "    threshold_score = percentiles_result[0]\n",
    "    print(\" threshold_score is -->\", threshold_score)\n",
    "\n",
    "    # filtered\n",
    "    docs = [doc for doc in docs_with_score if doc[1]< threshold_score]\n",
    "\n",
    "    # Fetching Prompts\n",
    "    docs_context = \"\"\n",
    "    for doc in docs:\n",
    "        docs_context = docs_context + f\" \\n\\\n",
    "        \\n\\\n",
    "            {doc[0].page_content}\"\n",
    "    prompt = Final_Prompt.replace(\"{context}\", docs_context).replace(\"{question}\", cust_query).replace(\"{leaves_mapping}\", str(leaves_mapping))\n",
    "    usr_df_loc = CONFIG['files_location']['usr_data_dir']    \n",
    "    print('The curated prompt is *****#####:', prompt)\n",
    "    prompt = inject_user_info(prompt=prompt, email=email, user_df_loc=usr_df_loc)\n",
    "\n",
    "    if 'azure' in CONFIG['model']['openai_api_type']:       \n",
    "        textdata_response = query_openai(prompt)\n",
    "        print(\" Azure Open AI Response --- \", textdata_response)\n",
    "    else:\n",
    "        BASE_LLM = ChatOpenAI(temperature=0, model = OpenAI_model)\n",
    "        # using OpenAI Base LLM\n",
    "        chain = load_qa_chain(BASE_LLM, chain_type=\"stuff\") \n",
    "        if outside_context ==\"yes\":\n",
    "            out_context_prompt = \"You will be given a question along with the context to answer below. \\n\\\n",
    "        You can use the information from your knowledge base or internet to answer the question in best possible manner. \\n\\\n",
    "            Give the grammatically correct and structured response. \\n\\\n",
    "            \"\n",
    "        else:\n",
    "            # check input_documents dtype (list/str)\n",
    "            textdata_response = chain.run(input_documents=docs, question=prompt)\n",
    "        print(\" Open AI response ---- \", textdata_response)\n",
    "\n",
    "    # Preparing Dynamic Citations\n",
    "    citation_texts = []\n",
    "    if outside_context==\"yes\":\n",
    "        pass\n",
    "    else:\n",
    "        for doc in docs:            \n",
    "            citation_texts.append([doc[0].page_content , doc[0].metadata, doc[1] ] )\n",
    "            #content, metadata, score\n",
    "        citations = []\n",
    "\n",
    "        for cit in citation_texts:\n",
    "            # better approach by checking if page is there\n",
    "            if cit[1].get(\"page\") is not None:\n",
    "                # to get just the filename instead of complete path\n",
    "                filename = re.search(r'[\\\\/]+([^\\\\/]+)$', cit[1][\"source\"])\n",
    "                # re.search(r'[\\\\/]+([^\\\\/]+)$', file)\n",
    "                print(\"filename label out \", filename)\n",
    "                if filename:\n",
    "                    filename = filename.group(1)\n",
    "                    print(\"filename label \", filename)\n",
    "                # added filename field for download_file API\n",
    "                citations.append({'filename':str(filename), 'label':str(filename) + \", Page : \" + str(cit[1][\"page\"] +1) , \"description\":cit[0], 'link':None, 'textToHighlight':None })\n",
    "            else:\n",
    "                filename = re.search(r'[\\\\/]+([^\\\\/]+)$', cit[1][\"source\"])\n",
    "                print(\"filename label out \", filename)\n",
    "                if filename:\n",
    "                    filename = filename.group(1)\n",
    "                    print(\"filename label \", filename)\n",
    "                citations.append({'filename':str(filename), 'label':str(filename) + \", Page : 0 \"  , \"description\":cit[0], 'link':None, 'textToHighlight':None })\n",
    "    # Remember the last two questions\n",
    "    last_two_questions.pop(0)\n",
    "    last_two_questions.append(cust_query)\n",
    "    return [cust_query, textdata_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_queru = 'What is CL'\n",
    "\n",
    "query_pdfs_intel(cus_queru, \"no\", \"pky@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = 'What is ml'\n",
    "q2 =  'how about consolidated revenue'\n",
    "p2 = p1.replace('{question_1}', q1).replace('{question_2}', q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = 'What is ml'\n",
    "q2 =  'what is consolidated revenue'\n",
    "p2 = p1.replace('{question_1}', q1).replace('{question_2}', q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config file:  config\\config.ini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain import OpenAI, ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationEntityMemory\n",
    "\n",
    "import hashlib\n",
    "\n",
    "from langchain.document_loaders import UnstructuredPowerPointLoader\n",
    "import pandas as pd\n",
    "from config.config_validator import validate_ini_file\n",
    "\n",
    "from utils import query_openai, query_openai_gpt35_backup, return_embeddings_model, create_document_loader, create_document_splitter, create_vector_store, inject_user_info, calculate_percentiles, get_file_types_csv\n",
    "from config.prompts import Final_Prompt, Context_Question_Prompt, leaves_mapping, new_template\n",
    "#from deploy import email\n",
    "\n",
    "from utils import list_directories_sorted_by_modified_time, query_openai\n",
    "from logger.logger_setup import logger\n",
    "import time\n",
    "import concurrent.futures\n",
    "import itertools\n",
    "CONFIG_LOC = os.environ.get('CONFIG_LOC', str(Path('config/config.ini')))\n",
    "print(\"Using config file: \", CONFIG_LOC)\n",
    "# validate and read config file:\n",
    "CONFIG = validate_ini_file(CONFIG_LOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The curated prompt is *****#####:  You're an assistance to a Human powered by nexus, designed to answer questions about nexus policies.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a custom_database provides to you. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Also you can use list '['SOL': 'Special Occasion Leave (SOL)', 'CL': 'Casual Leave (CL)', 'EL': 'Earned Leave (EL)', 'ML': 'Maternity Leave (ML)', 'PL': 'Paternity Leave (PL)', 'SL': 'Sick Leave (SL)', 'LWP': 'Leave Without Pay (LWP)']' for synonyms for leaves.\n",
      "You have to use below custom_database to answer questions:\n",
      "'India is Asian country. Its is a very beautiful country. It's national animal is Tiger. National game is Cricket'\n",
      "\n",
      "Some point to note while generating an answer:\n",
      "a) Be curteous and creative in your responses. Answer like you are a friendly agent, making the user feel comfortable.\n",
      "b) Be very structured in your response, Give the response with html tags as explained below\n",
      "give <b> html tags for bullet points and to bold the important words. Use <br> html tags to display contents in new lines wherever required for clear displaying. \n",
      "Important Note : We need to display the content with proper HTML tags. Your task is to automatically add the relevant <li>, <ul>, break <br>, <p>, bold <b> tags to beautifully present the responses to be displayed in an html <div> tag.\n",
      "c) Remember to not talk about any leaves that are not applicable to me. Example Maternity Leaves are not applicable to male employees. Contractual Leave is not valid for non contractual employeesS\n",
      "d) The answer should not be in the email format and look like a normal chat.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to the related questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "If the information provided is insufficient to answer the question or the question is incomplete or not clear, you can ask for the relevant question clarifying question with the information in prompt and your knwoledge. To ask clarifying question you can take help from last asked question by a user: 'last_question'.\n",
      "\n",
      "Context:\n",
      "{entities}\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Last line:\n",
      "Human: {input}\n",
      "You:', template_format='f-string', validate_template=True\n",
      "Injecting user information into the prompt...\n"
     ]
    }
   ],
   "source": [
    "docs_context = \"India is Asian country. Its is a very beautiful country. It's national animal is Tiger. National game is Cricket\"\n",
    "last_two_questions = ['What is india', 'what is national game of india']\n",
    "\n",
    "prompt = new_template.replace(\"custom_db\",docs_context).replace('leaves_mapping', str(leaves_mapping).replace('last_question', last_two_questions[1]).replace('{', '[').replace('}', ']'))\n",
    "usr_df_loc = CONFIG['files_location']['usr_data_dir']    \n",
    "print('The curated prompt is *****#####:', prompt)\n",
    "prompt = inject_user_info(prompt=prompt, email='vishal.chaudhari@absolutdata.com', user_df_loc=usr_df_loc)\n",
    "prompt_template = PromptTemplate(input_variables=['entities', 'history', 'input'], output_parser=None, partial_variables={}, template=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You're an assistance to a Human powered by nexus, designed to answer questions about nexus policies.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a custom_database provides to you. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Also you can use list '['SOL': 'Special Occasion Leave (SOL)', 'CL': 'Casual Leave (CL)', 'EL': 'Earned Leave (EL)', 'ML': 'Maternity Leave (ML)', 'PL': 'Paternity Leave (PL)', 'SL': 'Sick Leave (SL)', 'LWP': 'Leave Without Pay (LWP)']' for synonyms for leaves.\n",
      "You have to use below custom_database to answer questions:\n",
      "'India is Asian country. Its is a very beautiful country. It's national animal is Tiger. National game is Cricket'\n",
      "\n",
      "Some point to note while generating an answer:\n",
      "a) Be curteous and creative in your responses. Answer like you are a friendly agent, making the user feel comfortable.\n",
      "b) Be very structured in your response, Give the response with html tags as explained below\n",
      "give <b> html tags for bullet points and to bold the important words. Use <br> html tags to display contents in new lines wherever required for clear displaying. \n",
      "Important Note : We need to display the content with proper HTML tags. Your task is to automatically add the relevant <li>, <ul>, break <br>, <p>, bold <b> tags to beautifully present the responses to be displayed in an html <div> tag.\n",
      "c) Remember to not talk about any leaves that are not applicable to me. Example Maternity Leaves are not applicable to male employees. Contractual Leave is not valid for non contractual employeesS\n",
      "d) The answer should not be in the email format and look like a normal chat.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to the related questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "If the information provided is insufficient to answer the question or the question is incomplete or not clear, you can ask for the relevant question clarifying question with the information in prompt and your knwoledge. To ask clarifying question you can take help from last asked question by a user: 'last_question'.\n",
      "\n",
      "Context:\n",
      "{'India': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: What is india\n",
      "You:', template_format='f-string', validate_template=True\n",
      "Also, here is some useful information about the person asking the question: name is Vishal Chaudhari\n",
      "Gender is Male\n",
      "grade level is G2\n",
      "date_joined is 21-01-2022\n",
      "Earned Leave(EL) availed this year is 0\n",
      "Casual Leave(CL)/ Sick Leave(SL) availed this year is 5\n",
      " Maternity Leaves(ML) availed this year is Not Applicable\n",
      " Paternity Leaves(PL) availed this year is 5\n",
      "Total Trainings done is 3\n",
      "Total Training Hours is 20\n",
      "Total Training required is 10\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You: <div> Hello Vishal, <br> <p>India is a country located in the continent of Asia. It is known for its rich culture, diverse traditions, and scenic beauty. The national animal of India is the <b>Tiger</b> and the national game is <b>Cricket</b>. </p> <br> I hope this information is helpful to you. If you have any other questions, feel free to ask. </div>'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = CONFIG['azure_llm_instances']['model_name']\n",
    "deployment_names = CONFIG['azure_llm_instances']['deployment_name']\n",
    "ns = CONFIG['azure_llm_instances']['n']\n",
    "temperatures = CONFIG['azure_llm_instances']['temperature']\n",
    "openai_api_types = CONFIG['azure_llm_instances']['openai_api_type']\n",
    "openai_api_bases = CONFIG['azure_llm_instances']['openai_api_base']\n",
    "openai_api_versions = CONFIG['azure_llm_instances']['openai_api_version']\n",
    "openai_api_keys = CONFIG['azure_llm_instances']['openai_api_key']\n",
    "\n",
    "i==1\n",
    "llm = AzureChatOpenAI(model_name=model_names[i],\n",
    "            deployment_name=deployment_names[i],\n",
    "            n=ns[i],\n",
    "            temperature=temperatures[i],\n",
    "            openai_api_type = openai_api_types[i],\n",
    "            openai_api_base = openai_api_bases[i],\n",
    "            openai_api_version = openai_api_versions[i],\n",
    "            openai_api_key = openai_api_keys[i])\n",
    "    \n",
    "    #conversation = ConversationChain(llm=instances[1], verbose=True, prompt = prompt_template, memory=ConversationEntityMemory(llm=instances[1], k=3))\n",
    "conversation = ConversationChain(llm=llm, verbose=True, prompt = prompt_template, memory=ConversationEntityMemory(llm=llm, k=3))\n",
    "conversation.predict(input='What is india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You're an assistance to a Human powered by nexus, designed to answer questions about nexus policies.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a custom_database provides to you. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Also you can use list '['SOL': 'Special Occasion Leave (SOL)', 'CL': 'Casual Leave (CL)', 'EL': 'Earned Leave (EL)', 'ML': 'Maternity Leave (ML)', 'PL': 'Paternity Leave (PL)', 'SL': 'Sick Leave (SL)', 'LWP': 'Leave Without Pay (LWP)']' for synonyms for leaves.\n",
      "You have to use below custom_database to answer questions:\n",
      "'India is Asian country. Its is a very beautiful country. It's national animal is Tiger. National game is Cricket'\n",
      "\n",
      "Some point to note while generating an answer:\n",
      "a) Be curteous and creative in your responses. Answer like you are a friendly agent, making the user feel comfortable.\n",
      "b) Be very structured in your response, Give the response with html tags as explained below\n",
      "give <b> html tags for bullet points and to bold the important words. Use <br> html tags to display contents in new lines wherever required for clear displaying. \n",
      "Important Note : We need to display the content with proper HTML tags. Your task is to automatically add the relevant <li>, <ul>, break <br>, <p>, bold <b> tags to beautifully present the responses to be displayed in an html <div> tag.\n",
      "c) Remember to not talk about any leaves that are not applicable to me. Example Maternity Leaves are not applicable to male employees. Contractual Leave is not valid for non contractual employeesS\n",
      "d) The answer should not be in the email format and look like a normal chat.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to the related questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "If the information provided is insufficient to answer the question or the question is incomplete or not clear, you can ask for the relevant question clarifying question with the information in prompt and your knwoledge. To ask clarifying question you can take help from last asked question by a user: 'last_question'.\n",
      "\n",
      "Context:\n",
      "{'India': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: What is india\n",
      "You:', template_format='f-string', validate_template=True\n",
      "Also, here is some useful information about the person asking the question: name is Vishal Chaudhari\n",
      "Gender is Male\n",
      "grade level is G2\n",
      "date_joined is 21-01-2022\n",
      "Earned Leave(EL) availed this year is 0\n",
      "Casual Leave(CL)/ Sick Leave(SL) availed this year is 5\n",
      " Maternity Leaves(ML) availed this year is Not Applicable\n",
      " Paternity Leaves(PL) availed this year is 5\n",
      "Total Trainings done is 3\n",
      "Total Training Hours is 20\n",
      "Total Training required is 10\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You: Hello Vishal, <br>\\nIndia is a country located in the continent of Asia. It is known for its rich culture, diverse traditions, and scenic beauty. The national animal of India is the <b>Tiger</b> and the national game is <b>Cricket</b>.<br>\\nI hope this information is helpful to you. If you have any other questions, feel free to ask.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = CONFIG['azure_llm_instances']['model_name']\n",
    "deployment_names = CONFIG['azure_llm_instances']['deployment_name']\n",
    "ns = CONFIG['azure_llm_instances']['n']\n",
    "temperatures = CONFIG['azure_llm_instances']['temperature']\n",
    "openai_api_types = CONFIG['azure_llm_instances']['openai_api_type']\n",
    "openai_api_bases = CONFIG['azure_llm_instances']['openai_api_base']\n",
    "openai_api_versions = CONFIG['azure_llm_instances']['openai_api_version']\n",
    "openai_api_keys = CONFIG['azure_llm_instances']['openai_api_key']\n",
    "\n",
    "instances = [AzureChatOpenAI(model_name=model_names[1],\n",
    "            deployment_name=deployment_names[1],\n",
    "            n=ns[1],\n",
    "            temperature=temperatures[1],\n",
    "            openai_api_type = openai_api_types[1],\n",
    "            openai_api_base = openai_api_bases[1],\n",
    "            openai_api_version = openai_api_versions[1],\n",
    "            openai_api_key = openai_api_keys[1])]\n",
    "\n",
    "# for i in range(len(model_names)):\n",
    "#     llm = AzureChatOpenAI(model_name=model_names[i],\n",
    "#             deployment_name=deployment_names[i],\n",
    "#             n=ns[i],\n",
    "#             temperature=temperatures[i],\n",
    "#             openai_api_type = openai_api_types[i],\n",
    "#             openai_api_base = openai_api_bases[i],\n",
    "#             openai_api_version = openai_api_versions[i],\n",
    "#             openai_api_key = openai_api_keys[i])\n",
    "    \n",
    "#     instances.append(llm)\n",
    "\n",
    "conversation = ConversationChain(llm=instances[0], verbose=True, prompt = prompt_template, memory=ConversationEntityMemory(llm=instances[0], k=3))\n",
    "conversation.predict(input='What is india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'instances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(instances[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'instances' is not defined"
     ]
    }
   ],
   "source": [
    "print(instances[0])\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m cust_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is india\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m instances \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(model_names)):\n\u001b[0;32m      4\u001b[0m     llm \u001b[38;5;241m=\u001b[39m AzureChatOpenAI(model_name\u001b[38;5;241m=\u001b[39mmodel_names[i],\n\u001b[0;32m      5\u001b[0m             deployment_name\u001b[38;5;241m=\u001b[39mdeployment_names[i],\n\u001b[0;32m      6\u001b[0m             n\u001b[38;5;241m=\u001b[39mns[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m             openai_api_version \u001b[38;5;241m=\u001b[39m openai_api_versions[i],\n\u001b[0;32m     11\u001b[0m             openai_api_key \u001b[38;5;241m=\u001b[39m openai_api_keys[i])\n\u001b[0;32m     13\u001b[0m     instances\u001b[38;5;241m.\u001b[39mappend(llm)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_names' is not defined"
     ]
    }
   ],
   "source": [
    "cust_query = 'What is india'\n",
    "instances = []\n",
    "for i in range(len(model_names)):\n",
    "    llm = AzureChatOpenAI(model_name=model_names[i],\n",
    "            deployment_name=deployment_names[i],\n",
    "            n=ns[i],\n",
    "            temperature=temperatures[i],\n",
    "            openai_api_type = openai_api_types[i],\n",
    "            openai_api_base = openai_api_bases[i],\n",
    "            openai_api_version = openai_api_versions[i],\n",
    "            openai_api_key = openai_api_keys[i])\n",
    "    \n",
    "    instances.append(llm)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(instances)) as executor:\n",
    "    load_balancer = round_robin(instances)\n",
    "    for i in range(len(model_names)):\n",
    "        llm = next(load_balancer)\n",
    "        print(llm)\n",
    "        conversation = ConversationChain(llm=llm, verbose=True, prompt = prompt_template,\n",
    "                                            memory=ConversationEntityMemory(llm=llm, k=3))\n",
    "        break\n",
    "    futures = [executor.submit(conversation.predict, input=cust_query) for _ in range(len(instances))]\n",
    "    print(futures)\n",
    "    # Wait for the first successful response\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            textdata_response = future.result()\n",
    "#            logger.info(f'{email}, {correlation_id}, Query: {cust_query}, Query_Type: {\"Normal\"}, Execution_Time: {time.time() - start_time}, Status: {\"Query Executed Successfully\"}')\n",
    "            print(\" Azure Open AI Response --- \", textdata_response)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # Log the error and continue to the next instance\n",
    "#            logger.error(f'{email}, {correlation_id}, Query: {cust_query}, Query_Type: {\"Normal\"}, Execution_Time: {time.time() - start_time}, Status: {f\"Instance failed. Trying the next instance\"}')\n",
    "\n",
    "# # Use the round-robin strategy to select an instance for each request\n",
    "#     load_balancer = round_robin(instances)\n",
    "#     # Submit requests concurrently\n",
    "# for i in range(1):\n",
    "#     llm = next(load_balancer)\n",
    "#     print(llm)\n",
    "#     conversation = ConversationChain(llm, verbose=True, prompt = prompt_template,\n",
    "#                                     memory=ConversationEntityMemory(llm=llm, k=3))\n",
    "\n",
    "#futures = [executor.submit(conversation.predict, input=cust_query) for _ in range(len(instances))]\n",
    "# Wait for the first successful response\n",
    "# for future in concurrent.futures.as_completed(futures):\n",
    "#     try:\n",
    "#         textdata_response = future.result()\n",
    "# #                    logger.info(f'{email}, {correlation_id}, Query: {cust_query}, Query_Type: {\"Normal\"}, Execution_Time: {time.time() - start_time}, Status: {\"Query Executed Successfully\"}')\n",
    "#         print(\" Azure Open AI Response --- \", textdata_response)\n",
    "#         break\n",
    "#     except Exception as e:\n",
    "#         Log the error and continue to the next instance\n",
    "#                 logger.error(f'{email}, {correlation_id}, Query: {cust_query}, Query_Type: {\"Normal\"}, Execution_Time: {time.time() - start_time}, Status: {f\"Instance failed. Trying the next instance\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vishal.chaudhari.IGGLOBAL\\Documents\\ask-chatbot-dev-nexus\\test.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vishal.chaudhari.IGGLOBAL/Documents/chatbot-dev/test.ipynb#Y131sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m features\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_robin(iterable):\n",
    "    # Cycle through the iterable indefinitely\n",
    "    pool = itertools.cycle(iterable)\n",
    "    while True:\n",
    "        yield next(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "l = ['a', 'b', 'c']\n",
    "load_balancer = round_robin(l)\n",
    "for i in range(1):\n",
    "    j = next(load_balancer)\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One input key expected got []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vishal.chaudhari.IGGLOBAL\\Documents\\ask-chatbot-dev-nexus\\test.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vishal.chaudhari.IGGLOBAL/Documents/chatbot-dev/test.ipynb#Y115sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#conversation = ConversationChain(llm=instances[1], verbose=True, prompt = prompt_template, memory=ConversationEntityMemory(llm=instances[1], k=3))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vishal.chaudhari.IGGLOBAL/Documents/chatbot-dev/test.ipynb#Y115sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m conversation \u001b[39m=\u001b[39m ConversationChain(llm\u001b[39m=\u001b[39mllm, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prompt \u001b[39m=\u001b[39m prompt_template, memory\u001b[39m=\u001b[39mConversationEntityMemory(llm\u001b[39m=\u001b[39mllm, k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vishal.chaudhari.IGGLOBAL/Documents/chatbot-dev/test.ipynb#Y115sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m conversation\u001b[39m.\u001b[39;49mpredict(\u001b[39m'\u001b[39;49m\u001b[39mWhat is india\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\vishal.chaudhari.IGGLOBAL\\.conda\\envs\\test_play\\lib\\site-packages\\langchain\\chains\\llm.py:213\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    199\u001b[0m     \u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mc:\\Users\\vishal.chaudhari.IGGLOBAL\\.conda\\envs\\test_play\\lib\\site-packages\\langchain\\chains\\base.py:123\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[0;32m    107\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    108\u001b[0m     inputs: Union[Dict[\u001b[39mstr\u001b[39m, Any], Any],\n\u001b[0;32m    109\u001b[0m     return_only_outputs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    110\u001b[0m     callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    111\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m    112\u001b[0m     \u001b[39m\"\"\"Run the logic of this chain and add to output if desired.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \n\u001b[0;32m    114\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \n\u001b[0;32m    122\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_inputs(inputs)\n\u001b[0;32m    124\u001b[0m     callback_manager \u001b[39m=\u001b[39m CallbackManager\u001b[39m.\u001b[39mconfigure(\n\u001b[0;32m    125\u001b[0m         callbacks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[0;32m    126\u001b[0m     )\n\u001b[0;32m    127\u001b[0m     new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vishal.chaudhari.IGGLOBAL\\.conda\\envs\\test_play\\lib\\site-packages\\langchain\\chains\\base.py:214\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     inputs \u001b[39m=\u001b[39m {\u001b[39mlist\u001b[39m(_input_keys)[\u001b[39m0\u001b[39m]: inputs}\n\u001b[0;32m    213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m     external_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49mload_memory_variables(inputs)\n\u001b[0;32m    215\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mexternal_context)\n\u001b[0;32m    216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_inputs(inputs)\n",
      "File \u001b[1;32mc:\\Users\\vishal.chaudhari.IGGLOBAL\\.conda\\envs\\test_play\\lib\\site-packages\\langchain\\memory\\entity.py:272\u001b[0m, in \u001b[0;36mConversationEntityMemory.load_memory_variables\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    270\u001b[0m chain \u001b[39m=\u001b[39m LLMChain(llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm, prompt\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentity_extraction_prompt)\n\u001b[0;32m    271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 272\u001b[0m     prompt_input_key \u001b[39m=\u001b[39m get_prompt_input_key(inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory_variables)\n\u001b[0;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     prompt_input_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key\n",
      "File \u001b[1;32mc:\\Users\\vishal.chaudhari.IGGLOBAL\\.conda\\envs\\test_play\\lib\\site-packages\\langchain\\memory\\utils.py:11\u001b[0m, in \u001b[0;36mget_prompt_input_key\u001b[1;34m(inputs, memory_variables)\u001b[0m\n\u001b[0;32m      9\u001b[0m prompt_input_keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(inputs)\u001b[39m.\u001b[39mdifference(memory_variables \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[0;32m     10\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(prompt_input_keys) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOne input key expected got \u001b[39m\u001b[39m{\u001b[39;00mprompt_input_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m prompt_input_keys[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: One input key expected got []"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "i==1\n",
    "llm = AzureChatOpenAI(model_name=model_names[i],\n",
    "        deployment_name=deployment_names[i],\n",
    "        n=ns[i],\n",
    "        temperature=temperatures[i],\n",
    "        openai_api_type = openai_api_types[i],\n",
    "        openai_api_base = openai_api_bases[i],\n",
    "        openai_api_version = openai_api_versions[i],\n",
    "        openai_api_key = openai_api_keys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1628141753.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    AzureChatOpenAI(verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4', temperature=0.0, model_kwargs={}, openai_api_key='f6fb8d876xxxxxxxxx', openai_api_base='https://aspiregpt4.openai.azure.com/', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, deployment_name='chatbot-dev-nexusAspireGPT1', openai_api_type='azure', openai_api_version='2023-03-15-preview')\u001b[0m\n\u001b[1;37m                                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "AzureChatOpenAI(verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4', temperature=0.0, model_kwargs={}, openai_api_key='f6fb8d876xxxxxxxxx', openai_api_base='https://aspiregpt4.openai.azure.com/', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, deployment_name='chatbot-dev-nexusAspireGPT1', openai_api_type='azure', openai_api_version='2023-03-15-preview')\n",
    "\n",
    "AzureChatOpenAI(verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4-32k', temperature=0.0, model_kwargs={}, openai_api_key='f0dxxxxxxxxxxx', openai_api_base='https://askchatbot-dev-nexus-gpt-4.openai.azure.com/', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, deployment_name='gpt-4', openai_api_type='azure', openai_api_version='2023-03-15-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config file:  config/config.ini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain import OpenAI, ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationEntityMemory\n",
    "\n",
    "import hashlib\n",
    "\n",
    "from langchain.document_loaders import UnstructuredPowerPointLoader\n",
    "import pandas as pd\n",
    "from config.config_validator import validate_ini_file\n",
    "\n",
    "from utils import query_openai, query_openai_gpt35_backup, return_embeddings_model, create_document_loader, create_document_splitter, create_vector_store, inject_user_info, calculate_percentiles, get_file_types_csv\n",
    "from config.prompts import Final_Prompt, Context_Question_Prompt, leaves_mapping, new_template\n",
    "#from deploy import email\n",
    "\n",
    "from utils import list_directories_sorted_by_modified_time, query_openai\n",
    "from logger.logger_setup import logger\n",
    "import time\n",
    "import concurrent.futures\n",
    "import itertools\n",
    "CONFIG_LOC = os.environ.get('CONFIG_LOC', str(Path('config/config.ini')))\n",
    "print(\"Using config file: \", CONFIG_LOC)\n",
    "# validate and read config file:\n",
    "CONFIG = validate_ini_file(CONFIG_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The curated prompt is *****#####:  You're an assistance to a Human powered by nexus, designed to answer questions about nexus policies.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a custom_database provides to you. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Also you can use list '['SOL': 'Special Occasion Leave (SOL)', 'CL': 'Casual Leave (CL)', 'EL': 'Earned Leave (EL)', 'ML': 'Maternity Leave (ML)', 'PL': 'Paternity Leave (PL)', 'SL': 'Sick Leave (SL)', 'LWP': 'Leave Without Pay (LWP)']' for synonyms for leaves.\n",
      "You have to use below custom_database to answer questions:\n",
      "'India is Asian country. Its is a very beautiful country. It's national animal is Tiger. National game is Cricket'\n",
      "\n",
      "Some point to note while generating an answer:\n",
      "a) Be curteous and creative in your responses. Answer like you are a friendly agent, making the user feel comfortable.\n",
      "b) Be very structured in your response, Give the response with html tags as explained below\n",
      "give <b> html tags for bullet points and to bold the important words. Use <br> html tags to display contents in new lines wherever required for clear displaying. \n",
      "Important Note : We need to display the content with proper HTML tags. Your task is to automatically add the relevant <li>, <ul>, break <br>, <p>, bold <b> tags to beautifully present the responses to be displayed in an html <div> tag.\n",
      "c) Remember to not talk about any leaves that are not applicable to me. Example Maternity Leaves are not applicable to male employees. Contractual Leave is not valid for non contractual employeesS\n",
      "d) The answer should not be in the email format and look like a normal chat.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to the related questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "If the information provided is insufficient to answer the question or the question is incomplete or not clear, you can ask for the relevant question clarifying question with the information in prompt and your knwoledge. To ask clarifying question you can take help from last asked question by a user: 'last_question'.\n",
      "\n",
      "Context:\n",
      "{entities}\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Last line:\n",
      "Human: {input}\n",
      "You:', template_format='f-string', validate_template=True\n",
      "Injecting user information into the prompt...\n"
     ]
    }
   ],
   "source": [
    "docs_context = \"India is Asian country. Its is a very beautiful country. It's national animal is Tiger. National game is Cricket\"\n",
    "last_two_questions = ['What is india', 'what is national game of india']\n",
    "\n",
    "prompt = new_template.replace(\"custom_db\",docs_context).replace('leaves_mapping', str(leaves_mapping).replace('last_question', last_two_questions[1]).replace('{', '[').replace('}', ']'))\n",
    "usr_df_loc = CONFIG['files_location']['usr_data_dir']    \n",
    "print('The curated prompt is *****#####:', prompt)\n",
    "prompt = inject_user_info(prompt=prompt, email='pky@gmail.com', user_df_loc=usr_df_loc)\n",
    "prompt_template = PromptTemplate(input_variables=['entities', 'history', 'input'], output_parser=None, partial_variables={}, template=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_instance_index = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_query = 'What is sport of india'\n",
    "#cust_query = 'Who is Prime Minister if India' \n",
    "#cust_query = 'What is india?'\n",
    "#cust_query = 'Where is India?'\n",
    "#cust_query = 'What is national bird of india'\n",
    "email = 'pky@gmail.com'\n",
    "correlation_id = '1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the llm being used is:  0\n",
      "error is #### Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.\n",
      "the llm being used is:  1\n",
      "error is #### Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.\n",
      "the llm being used is:  2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You're an assistance to a Human powered by nexus, designed to answer questions about nexus policies.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a custom_database provides to you. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Also you can use list '['SOL': 'Special Occasion Leave (SOL)', 'CL': 'Casual Leave (CL)', 'EL': 'Earned Leave (EL)', 'ML': 'Maternity Leave (ML)', 'PL': 'Paternity Leave (PL)', 'SL': 'Sick Leave (SL)', 'LWP': 'Leave Without Pay (LWP)']' for synonyms for leaves.\n",
      "You have to use below custom_database to answer questions:\n",
      "'India is Asian country. Its is a very beautiful country. It's national animal is Tiger. National game is Cricket'\n",
      "\n",
      "Some point to note while generating an answer:\n",
      "a) Be curteous and creative in your responses. Answer like you are a friendly agent, making the user feel comfortable.\n",
      "b) Be very structured in your response, Give the response with html tags as explained below\n",
      "give <b> html tags for bullet points and to bold the important words. Use <br> html tags to display contents in new lines wherever required for clear displaying. \n",
      "Important Note : We need to display the content with proper HTML tags. Your task is to automatically add the relevant <li>, <ul>, break <br>, <p>, bold <b> tags to beautifully present the responses to be displayed in an html <div> tag.\n",
      "c) Remember to not talk about any leaves that are not applicable to me. Example Maternity Leaves are not applicable to male employees. Contractual Leave is not valid for non contractual employeesS\n",
      "d) The answer should not be in the email format and look like a normal chat.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to the related questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "If the information provided is insufficient to answer the question or the question is incomplete or not clear, you can ask for the relevant question clarifying question with the information in prompt and your knwoledge. To ask clarifying question you can take help from last asked question by a user: 'last_question'.\n",
      "\n",
      "Context:\n",
      "{'India': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: What is sport of india\n",
      "You:', template_format='f-string', validate_template=True\n",
      "Also, here is some useful information about the person asking the question: name is Vishal Chaudhari\n",
      "Gender is Male\n",
      "grade level is G2\n",
      "date_joined is 21-01-2022\n",
      "Earned Leave(EL) availed this year is 0\n",
      "Casual Leave(CL)/ Sick Leave(SL) availed this year is 5\n",
      " Maternity Leaves(ML) availed this year is Not Applicable\n",
      " Paternity Leaves(PL) availed this year is 5\n",
      "Total Trainings done is 3\n",
      "Total Training Hours is 20\n",
      "Total Training required is 10\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Azure Open AI Response ---  You: Hello Vishal, <br> The national sport of India is <b>Cricket</b>. It's a sport that is immensely popular and widely followed in the country. <br> I hope this answers your question. If you have any more queries, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "model_names = CONFIG['azure_llm_instances']['model_name']\n",
    "deployment_names = CONFIG['azure_llm_instances']['deployment_name']\n",
    "ns = CONFIG['azure_llm_instances']['n']\n",
    "temperatures = CONFIG['azure_llm_instances']['temperature']\n",
    "openai_api_types = CONFIG['azure_llm_instances']['openai_api_type']\n",
    "openai_api_bases = CONFIG['azure_llm_instances']['openai_api_base']\n",
    "openai_api_versions = CONFIG['azure_llm_instances']['openai_api_version']\n",
    "openai_api_keys = CONFIG['azure_llm_instances']['openai_api_key']\n",
    "instances = []\n",
    "start_time = time.time()\n",
    "for i in range(len(model_names)):\n",
    "    llm = AzureChatOpenAI(model_name=model_names[i],\n",
    "            deployment_name=deployment_names[i],\n",
    "            n=ns[i],\n",
    "            temperature=temperatures[i],\n",
    "            openai_api_type = openai_api_types[i],\n",
    "            openai_api_base = openai_api_bases[i],\n",
    "            openai_api_version = openai_api_versions[i],\n",
    "            openai_api_key = openai_api_keys[i])           \n",
    "    instances.append(llm)\n",
    "\n",
    "conversation = ConversationChain(llm=instances[current_instance_index[0]], verbose=True, prompt = prompt_template,\n",
    "                                        memory=ConversationEntityMemory(llm=instances[current_instance_index[0]], k=3))\n",
    "\n",
    "max_retires = 2\n",
    "for _ in range(max_retires + 1):\n",
    "    try:\n",
    "        conversation = ConversationChain(llm=instances[current_instance_index[0]], verbose=True, prompt = prompt_template,\n",
    "                                                    memory=ConversationEntityMemory(llm=instances[current_instance_index[0]], k=3))\n",
    "        print(\"the llm being used is: \", current_instance_index[0])\n",
    "        textdata_response = conversation.predict(input = cust_query)\n",
    "        logger.info(f'{email}, {correlation_id}, Query: {cust_query}, Query_Type: {\"Normal\"}, Execution_Time: {time.time() - start_time}, Status: {\"Query Executed Successfully\"}')\n",
    "        print(\" Azure Open AI Response --- \", textdata_response)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        # Logging the error and continue to the next instance\n",
    "        print('error is ####', e)\n",
    "        current_instance_index[0] = (current_instance_index[0] + 1) % len(instances)\n",
    "        logger.error(f'{email}, {correlation_id}, Query: {cust_query}, Query_Type: {\"Normal\"}, Execution_Time: {time.time() - start_time}, Status: {f\"Instance failed. Trying the next instance\"}')\n",
    "else:\n",
    "    # Logging the errir as all retries failed\n",
    "    logger.error(f'{email}, {correlation_id}, Query: {cust_query}, Query_Type: {\"Normal\"}, Execution_Time: {time.time() - start_time}, Status: {\"All instances failed\"}')        \n",
    "current_instance_index[0] = (current_instance_index[0] + 1) % len(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = 'xxx@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_greeting_general = \"\"\"\n",
    "If the question is more of general conversation, like the user is greeting you/asking a general question or greeting you with hi or hello or heyy then respond back with the general greeting with the name wherever possible and use name from (str({email}.split('@')[0].split('.')[0] + ' ' + {email}.split('@')[0].split('.')[1]). also, here is some information that you may find useful for certain questions like 'Who are you?', 'What can you do?'': '{CON}'.\\n\\\n",
    "    5. If the question seems gratitude, like the user is thanking you or expressing gratitude with thank you or thanks or thank you so much then respond back with My Pleasure. Glad to help you. If you have any further question then please feel free to ask. If the user wants to end the conversation like saying bye or by or bye-bye or see you later or catch you later or I am done then respond with Bye!!  \\n\\\n",
    "    \\n\\\n",
    "Now the main Question to answer is:\\n\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIf the question is more of general conversation, like the user is greeting you/asking a general question or greeting you with hi or hello or heyy then respond back with the general greeting with the name wherever possible and use name from (str({email}.split('@')[0].split('.')[0] + ' ' + {email}.split('@')[0].split('.')[1]). also, here is some information that you may find useful for certain questions like 'Who are you?', 'What can you do?'': '{CON}'.\\n    5. If the question seems gratitude, like the user is thanking you or expressing gratitude with thank you or thanks or thank you so much then respond back with My Pleasure. Glad to help you. If you have any further question then please feel free to ask. If the user wants to end the conversation like saying bye or by or bye-bye or see you later or catch you later or I am done then respond with Bye!!  \\n    \\nNow the main Question to answer is:\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_greeting_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
