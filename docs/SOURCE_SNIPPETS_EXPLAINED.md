# Source Snippets â€” How It Works & How to Explain in an Interview

## What It Is (One-Liner for Interviewer)

**"Each RAG answer shows which documents were used and a short snippet from each, so the user can see where the answer came from and verify or open the source."**

---

## End-to-End Flow (Technical)

1. **User** sends `/ask What is the leave policy?`
2. **RAG pipeline** (`doc_intel_search.query_pdfs_intel`):
   - Embeds the query, runs FAISS similarity search over the knowledge base
   - Takes top chunks (by score threshold)
   - Sends those chunks + query to the LLM â†’ gets the answer text
   - Builds a **citations** list: for each chunk, `{ "filename", "label", "description" }`  
     - `filename` = source file (e.g. `leave_policy.pdf`)  
     - `description` = the actual chunk text (snippet) used as context
3. **Telegram bot** (`telegram_bot.cmd_ask`):
   - Receives the RAG response (answer + citations)
   - Calls `_format_source_snippets(citations)` to turn each citation into one line:  
     **â€¢ filename â€” "snippet..."**
   - Truncates each snippet to 180 characters so the message stays readable
   - Appends this block below the answer and sends it to the user

So: **citations come from the RAG layer (which doc/chunk was used); the bot only formats and displays them.**

---

## Where in the Code

| Step | File | What to point to |
|------|------|-------------------|
| Citations are **created** | `doc_intel_search.py` | Around lines 206â€“222: loop over `docs`, build `citations` with `filename`, `label`, `description` (= chunk `page_content`) |
| Citations are **formatted** | `telegram_bot.py` | `_format_source_snippets()` (around lines 52â€“62): for each citation, take `filename` and `description`, truncate description to 180 chars, output `â€¢ filename â€” "snippet..."` |
| Snippets are **attached** to reply | `telegram_bot.py` | In `cmd_ask`: after getting `result` from RAG (or from cache), `snippets = _format_source_snippets(citations)`, then `text = f"{text}\n\n{snippets}"` before sending |

---

## How to Check / Demo for the Interviewer

1. **Run the bot** (from `chatbot-dev`):  
   `python telegram_bot.py`
2. In Telegram, send:  
   `/ask What is the leave policy?`  
   (or any question that hits your uploaded docs)
3. In the reply you should see:
   - The answer text first
   - Then a block **"ðŸ“Ž Source snippets:"**
   - Under it, lines like:  
     **â€¢ leave_policy.pdf â€” "Employees are entitled to 12 days casual leave per year..."**
4. **Tell the interviewer:**  
   "The answer is generated by RAG from our vector store. The source snippets block shows which document and which part of it was used for the answer, so users can verify the source or open the file."

---

## Why We Truncate to 180 Characters

- Keeps the Telegram message short and readable
- Full chunk text is often long; 180 chars is enough to recognize the source and a bit of context
- Truncation is done in `_format_source_snippets` with `max_snippet_len=180`, cutting at a word boundary so we donâ€™t break mid-word

---

## Short Script for Saying It Out Loud

You can say something like:

*"For every RAG answer we donâ€™t just return the modelâ€™s reply â€” we also return **source snippets**. So for each piece of context we retrieved from the vector store, we show the **filename** and a **short snippet** of that chunk. That way the user can see exactly which document and which part of it the answer is based on. The citations are built in the RAG layer when we do the similarity search and build context for the LLM; in the Telegram bot we just format them into a clear list under the answer, with snippets truncated to 180 characters so the message stays readable."*
